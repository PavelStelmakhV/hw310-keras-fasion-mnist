{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOrKqxjBnEDiDzVyPauYiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavelStelmakhV/hw310-keras-fasion-mnist/blob/main/keras_fasion_mnist_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "vv7w0bgAVQPO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tf_keras\n",
        "\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras import callbacks\n",
        "from keras import initializers\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "mOpywsNAzfGB"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = y_train.astype('float32')\n",
        "y_test = y_test.astype('float32')"
      ],
      "metadata": {
        "id": "t2gucjsJ0KUi"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_init_tanh = initializers.glorot_normal(seed=111)\n",
        "# w_init_relu = initializers.HeNormal(seed=66)\n",
        "w_init_relu = initializers.HeUniform(seed=24)\n",
        "b_init = initializers.Zeros()"
      ],
      "metadata": {
        "id": "yL9QTEtBGy4N"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_out = 0.33"
      ],
      "metadata": {
        "id": "flsDcVTnU10J"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_relu(model, neurons):\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(layers.Dense(neurons,\n",
        "                       activation='relu',\n",
        "                       kernel_initializer=w_init_relu,\n",
        "                       bias_initializer=b_init))\n",
        "  model.add(layers.Dropout(drop_out))\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZZgs_25rKBCp"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_tanh(model, neurons):\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(layers.Dense(neurons,\n",
        "                        activation='tanh',\n",
        "                        kernel_initializer=w_init_tanh,\n",
        "                        bias_initializer=b_init))\n",
        "  model.add(layers.Dropout(drop_out))\n",
        "  return model"
      ],
      "metadata": {
        "id": "90JLLnkBR50t"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Va9BaQ9eG_uz"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "neurons = 256\n",
        "\n",
        "model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(layers.Dropout(0.25))\n",
        "\n",
        "model = layer_relu(model, neurons*1)\n",
        "model = layer_relu(model, neurons*2)\n",
        "model = layer_relu(model, neurons*2)\n",
        "# model = layer_relu(model, neurons*1)\n",
        "\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "atS39i4d0aqW"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
      ],
      "metadata": {
        "id": "tZO7hl7pM63y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.0003,\n",
        "                                      beta_1=0.92,\n",
        "                                      beta_2=0.999,\n",
        "                                      epsilon=1e-04\n",
        ")"
      ],
      "metadata": {
        "id": "EzfnaqIZlfy8"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
      ],
      "metadata": {
        "id": "KHmBptvOM95Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(#optimizer='adam',\n",
        "              # optimizer='adamax',\n",
        "              optimizer=optimizer,\n",
        "              # optimizer='rmsprop',\n",
        "              # optimizer='sgd',\n",
        "              # optimizer='adadelta',\n",
        "              # loss='binary_crossentropy',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy']) # sparse_categorical_accuracy"
      ],
      "metadata": {
        "id": "hENVaAC7ilNY"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.random((1, 28, 28))\n",
        "a = model.predict(x)\n",
        "_ = plt.hist(np.transpose(a))"
      ],
      "metadata": {
        "id": "RFgZHNNFMa8p",
        "outputId": "df83c80b-89d4-4d7f-9daa-210de65a44ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 119ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "UDP8OKKZ8oQa",
        "outputId": "eb25f208-8927-4bae-8a77-a91f50131a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                   patience=50,\n",
        "                                   restore_best_weights=True,\n",
        "                                   )\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=60)\n",
        "mc = callbacks.ModelCheckpoint('best_model.h5', monitor='accuracy', mode='max', verbose=0, save_best_only=True)\n"
      ],
      "metadata": {
        "id": "5ihU4exVlsjm"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=400,\n",
        "                    batch_size=128,\n",
        "                    # callbacks=[callback],\n",
        "                    callbacks=[mc, es],\n",
        "                    verbose=1, #многословие\n",
        "                    validation_split=0.1\n",
        "                    # validation_data=(x_val, y_val)\n",
        "                    )\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "FH8HcMNm233I",
        "outputId": "dc224035-f8f6-464a-8ecf-e64ebf090a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "422/422 [==============================] - 48s 30ms/step - loss: 0.9945 - accuracy: 0.6568 - val_loss: 0.4931 - val_accuracy: 0.8158\n",
            "Epoch 2/400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422/422 [==============================] - 13s 31ms/step - loss: 0.6453 - accuracy: 0.7668 - val_loss: 0.4360 - val_accuracy: 0.8380\n",
            "Epoch 3/400\n",
            "422/422 [==============================] - 12s 30ms/step - loss: 0.5728 - accuracy: 0.7923 - val_loss: 0.4207 - val_accuracy: 0.8430\n",
            "Epoch 4/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.5267 - accuracy: 0.8058 - val_loss: 0.3961 - val_accuracy: 0.8527\n",
            "Epoch 5/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.4971 - accuracy: 0.8184 - val_loss: 0.3890 - val_accuracy: 0.8582\n",
            "Epoch 6/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.4780 - accuracy: 0.8234 - val_loss: 0.3836 - val_accuracy: 0.8603\n",
            "Epoch 7/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.4612 - accuracy: 0.8280 - val_loss: 0.3728 - val_accuracy: 0.8607\n",
            "Epoch 8/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.4504 - accuracy: 0.8346 - val_loss: 0.3622 - val_accuracy: 0.8665\n",
            "Epoch 9/400\n",
            "422/422 [==============================] - 10s 25ms/step - loss: 0.4359 - accuracy: 0.8382 - val_loss: 0.3540 - val_accuracy: 0.8693\n",
            "Epoch 10/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.4237 - accuracy: 0.8410 - val_loss: 0.3555 - val_accuracy: 0.8675\n",
            "Epoch 11/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.4130 - accuracy: 0.8476 - val_loss: 0.3411 - val_accuracy: 0.8745\n",
            "Epoch 12/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.4047 - accuracy: 0.8509 - val_loss: 0.3436 - val_accuracy: 0.8748\n",
            "Epoch 13/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3992 - accuracy: 0.8522 - val_loss: 0.3305 - val_accuracy: 0.8812\n",
            "Epoch 14/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3928 - accuracy: 0.8533 - val_loss: 0.3273 - val_accuracy: 0.8783\n",
            "Epoch 15/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.3855 - accuracy: 0.8563 - val_loss: 0.3231 - val_accuracy: 0.8812\n",
            "Epoch 16/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3800 - accuracy: 0.8576 - val_loss: 0.3196 - val_accuracy: 0.8842\n",
            "Epoch 17/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.3752 - accuracy: 0.8601 - val_loss: 0.3154 - val_accuracy: 0.8842\n",
            "Epoch 18/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3691 - accuracy: 0.8623 - val_loss: 0.3193 - val_accuracy: 0.8840\n",
            "Epoch 19/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3639 - accuracy: 0.8655 - val_loss: 0.3175 - val_accuracy: 0.8848\n",
            "Epoch 20/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3613 - accuracy: 0.8655 - val_loss: 0.3249 - val_accuracy: 0.8808\n",
            "Epoch 21/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3583 - accuracy: 0.8657 - val_loss: 0.3113 - val_accuracy: 0.8870\n",
            "Epoch 22/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3568 - accuracy: 0.8658 - val_loss: 0.3099 - val_accuracy: 0.8865\n",
            "Epoch 23/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3512 - accuracy: 0.8677 - val_loss: 0.2985 - val_accuracy: 0.8893\n",
            "Epoch 24/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.3460 - accuracy: 0.8690 - val_loss: 0.3000 - val_accuracy: 0.8897\n",
            "Epoch 25/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3431 - accuracy: 0.8721 - val_loss: 0.2998 - val_accuracy: 0.8905\n",
            "Epoch 26/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3424 - accuracy: 0.8711 - val_loss: 0.3038 - val_accuracy: 0.8912\n",
            "Epoch 27/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3375 - accuracy: 0.8734 - val_loss: 0.2985 - val_accuracy: 0.8902\n",
            "Epoch 28/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.3369 - accuracy: 0.8737 - val_loss: 0.2922 - val_accuracy: 0.8917\n",
            "Epoch 29/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3326 - accuracy: 0.8749 - val_loss: 0.2959 - val_accuracy: 0.8910\n",
            "Epoch 30/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3309 - accuracy: 0.8762 - val_loss: 0.2968 - val_accuracy: 0.8900\n",
            "Epoch 31/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3258 - accuracy: 0.8773 - val_loss: 0.2957 - val_accuracy: 0.8932\n",
            "Epoch 32/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3242 - accuracy: 0.8781 - val_loss: 0.2894 - val_accuracy: 0.8908\n",
            "Epoch 33/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.3186 - accuracy: 0.8787 - val_loss: 0.2892 - val_accuracy: 0.8925\n",
            "Epoch 34/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3212 - accuracy: 0.8795 - val_loss: 0.2923 - val_accuracy: 0.8905\n",
            "Epoch 35/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3194 - accuracy: 0.8802 - val_loss: 0.2866 - val_accuracy: 0.8932\n",
            "Epoch 36/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3146 - accuracy: 0.8820 - val_loss: 0.2867 - val_accuracy: 0.8930\n",
            "Epoch 37/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3160 - accuracy: 0.8808 - val_loss: 0.2886 - val_accuracy: 0.8945\n",
            "Epoch 38/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3100 - accuracy: 0.8821 - val_loss: 0.2851 - val_accuracy: 0.8947\n",
            "Epoch 39/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3152 - accuracy: 0.8805 - val_loss: 0.2809 - val_accuracy: 0.8943\n",
            "Epoch 40/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3088 - accuracy: 0.8829 - val_loss: 0.2835 - val_accuracy: 0.8915\n",
            "Epoch 41/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.3070 - accuracy: 0.8827 - val_loss: 0.2823 - val_accuracy: 0.8948\n",
            "Epoch 42/400\n",
            "422/422 [==============================] - 10s 25ms/step - loss: 0.3051 - accuracy: 0.8852 - val_loss: 0.2842 - val_accuracy: 0.8972\n",
            "Epoch 43/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.3058 - accuracy: 0.8845 - val_loss: 0.2859 - val_accuracy: 0.8930\n",
            "Epoch 44/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3039 - accuracy: 0.8853 - val_loss: 0.2790 - val_accuracy: 0.8953\n",
            "Epoch 45/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3039 - accuracy: 0.8838 - val_loss: 0.2791 - val_accuracy: 0.8975\n",
            "Epoch 46/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3018 - accuracy: 0.8858 - val_loss: 0.2884 - val_accuracy: 0.8918\n",
            "Epoch 47/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2963 - accuracy: 0.8857 - val_loss: 0.2773 - val_accuracy: 0.8958\n",
            "Epoch 48/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.3003 - accuracy: 0.8886 - val_loss: 0.2777 - val_accuracy: 0.8965\n",
            "Epoch 49/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2974 - accuracy: 0.8873 - val_loss: 0.2781 - val_accuracy: 0.8998\n",
            "Epoch 50/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2946 - accuracy: 0.8889 - val_loss: 0.2771 - val_accuracy: 0.8953\n",
            "Epoch 51/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2952 - accuracy: 0.8883 - val_loss: 0.2748 - val_accuracy: 0.8985\n",
            "Epoch 52/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2934 - accuracy: 0.8866 - val_loss: 0.2781 - val_accuracy: 0.8960\n",
            "Epoch 53/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2935 - accuracy: 0.8895 - val_loss: 0.2789 - val_accuracy: 0.8992\n",
            "Epoch 54/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.2939 - accuracy: 0.8890 - val_loss: 0.2788 - val_accuracy: 0.8968\n",
            "Epoch 55/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2882 - accuracy: 0.8920 - val_loss: 0.2764 - val_accuracy: 0.8978\n",
            "Epoch 56/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2854 - accuracy: 0.8913 - val_loss: 0.2748 - val_accuracy: 0.8968\n",
            "Epoch 57/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2890 - accuracy: 0.8896 - val_loss: 0.2752 - val_accuracy: 0.8963\n",
            "Epoch 58/400\n",
            "422/422 [==============================] - 10s 24ms/step - loss: 0.2880 - accuracy: 0.8903 - val_loss: 0.2811 - val_accuracy: 0.8968\n",
            "Epoch 59/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2873 - accuracy: 0.8919 - val_loss: 0.2726 - val_accuracy: 0.8977\n",
            "Epoch 60/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2860 - accuracy: 0.8913 - val_loss: 0.2704 - val_accuracy: 0.8977\n",
            "Epoch 61/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2816 - accuracy: 0.8931 - val_loss: 0.2715 - val_accuracy: 0.8993\n",
            "Epoch 62/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2838 - accuracy: 0.8914 - val_loss: 0.2727 - val_accuracy: 0.8993\n",
            "Epoch 63/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2818 - accuracy: 0.8940 - val_loss: 0.2736 - val_accuracy: 0.8997\n",
            "Epoch 64/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2812 - accuracy: 0.8941 - val_loss: 0.2719 - val_accuracy: 0.9002\n",
            "Epoch 65/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2810 - accuracy: 0.8930 - val_loss: 0.2709 - val_accuracy: 0.9022\n",
            "Epoch 66/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2791 - accuracy: 0.8953 - val_loss: 0.2713 - val_accuracy: 0.9005\n",
            "Epoch 67/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.2813 - accuracy: 0.8935 - val_loss: 0.2741 - val_accuracy: 0.8972\n",
            "Epoch 68/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2779 - accuracy: 0.8940 - val_loss: 0.2730 - val_accuracy: 0.8992\n",
            "Epoch 69/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2763 - accuracy: 0.8949 - val_loss: 0.2685 - val_accuracy: 0.9003\n",
            "Epoch 70/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2738 - accuracy: 0.8961 - val_loss: 0.2716 - val_accuracy: 0.9002\n",
            "Epoch 71/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2747 - accuracy: 0.8957 - val_loss: 0.2686 - val_accuracy: 0.9003\n",
            "Epoch 72/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2737 - accuracy: 0.8952 - val_loss: 0.2715 - val_accuracy: 0.9002\n",
            "Epoch 73/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2756 - accuracy: 0.8963 - val_loss: 0.2712 - val_accuracy: 0.9005\n",
            "Epoch 74/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2737 - accuracy: 0.8964 - val_loss: 0.2721 - val_accuracy: 0.9012\n",
            "Epoch 75/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2738 - accuracy: 0.8967 - val_loss: 0.2656 - val_accuracy: 0.9027\n",
            "Epoch 76/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2704 - accuracy: 0.8960 - val_loss: 0.2695 - val_accuracy: 0.9012\n",
            "Epoch 77/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2713 - accuracy: 0.8958 - val_loss: 0.2661 - val_accuracy: 0.9028\n",
            "Epoch 78/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2673 - accuracy: 0.8987 - val_loss: 0.2648 - val_accuracy: 0.9020\n",
            "Epoch 79/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2708 - accuracy: 0.8972 - val_loss: 0.2663 - val_accuracy: 0.9018\n",
            "Epoch 80/400\n",
            "422/422 [==============================] - 13s 30ms/step - loss: 0.2665 - accuracy: 0.8984 - val_loss: 0.2705 - val_accuracy: 0.9007\n",
            "Epoch 81/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2674 - accuracy: 0.8973 - val_loss: 0.2686 - val_accuracy: 0.9022\n",
            "Epoch 82/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2671 - accuracy: 0.9002 - val_loss: 0.2749 - val_accuracy: 0.8977\n",
            "Epoch 83/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2673 - accuracy: 0.8977 - val_loss: 0.2719 - val_accuracy: 0.9038\n",
            "Epoch 84/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2663 - accuracy: 0.8975 - val_loss: 0.2656 - val_accuracy: 0.9043\n",
            "Epoch 85/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2657 - accuracy: 0.8979 - val_loss: 0.2689 - val_accuracy: 0.9042\n",
            "Epoch 86/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2654 - accuracy: 0.8979 - val_loss: 0.2694 - val_accuracy: 0.9033\n",
            "Epoch 87/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2655 - accuracy: 0.8985 - val_loss: 0.2710 - val_accuracy: 0.9008\n",
            "Epoch 88/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2631 - accuracy: 0.8989 - val_loss: 0.2657 - val_accuracy: 0.9020\n",
            "Epoch 89/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2622 - accuracy: 0.8997 - val_loss: 0.2660 - val_accuracy: 0.9033\n",
            "Epoch 90/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2625 - accuracy: 0.8991 - val_loss: 0.2633 - val_accuracy: 0.9042\n",
            "Epoch 91/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2635 - accuracy: 0.8989 - val_loss: 0.2674 - val_accuracy: 0.9035\n",
            "Epoch 92/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2606 - accuracy: 0.9006 - val_loss: 0.2647 - val_accuracy: 0.9045\n",
            "Epoch 93/400\n",
            "422/422 [==============================] - 12s 30ms/step - loss: 0.2605 - accuracy: 0.9009 - val_loss: 0.2689 - val_accuracy: 0.9030\n",
            "Epoch 94/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2590 - accuracy: 0.9009 - val_loss: 0.2704 - val_accuracy: 0.9007\n",
            "Epoch 95/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2607 - accuracy: 0.9006 - val_loss: 0.2696 - val_accuracy: 0.9007\n",
            "Epoch 96/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2550 - accuracy: 0.9018 - val_loss: 0.2673 - val_accuracy: 0.9030\n",
            "Epoch 97/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2610 - accuracy: 0.9016 - val_loss: 0.2690 - val_accuracy: 0.9038\n",
            "Epoch 98/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2587 - accuracy: 0.9011 - val_loss: 0.2668 - val_accuracy: 0.9038\n",
            "Epoch 99/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2574 - accuracy: 0.9020 - val_loss: 0.2646 - val_accuracy: 0.9047\n",
            "Epoch 100/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2570 - accuracy: 0.9027 - val_loss: 0.2641 - val_accuracy: 0.9047\n",
            "Epoch 101/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2581 - accuracy: 0.9014 - val_loss: 0.2647 - val_accuracy: 0.9030\n",
            "Epoch 102/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2551 - accuracy: 0.9029 - val_loss: 0.2654 - val_accuracy: 0.9050\n",
            "Epoch 103/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2567 - accuracy: 0.9034 - val_loss: 0.2683 - val_accuracy: 0.9043\n",
            "Epoch 104/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2564 - accuracy: 0.9024 - val_loss: 0.2611 - val_accuracy: 0.9043\n",
            "Epoch 105/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2533 - accuracy: 0.9025 - val_loss: 0.2617 - val_accuracy: 0.9048\n",
            "Epoch 106/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.2515 - accuracy: 0.9029 - val_loss: 0.2662 - val_accuracy: 0.9022\n",
            "Epoch 107/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2540 - accuracy: 0.9031 - val_loss: 0.2654 - val_accuracy: 0.9043\n",
            "Epoch 108/400\n",
            "422/422 [==============================] - 10s 25ms/step - loss: 0.2522 - accuracy: 0.9033 - val_loss: 0.2661 - val_accuracy: 0.9038\n",
            "Epoch 109/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2552 - accuracy: 0.9022 - val_loss: 0.2633 - val_accuracy: 0.9045\n",
            "Epoch 110/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2519 - accuracy: 0.9036 - val_loss: 0.2665 - val_accuracy: 0.9020\n",
            "Epoch 111/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2516 - accuracy: 0.9039 - val_loss: 0.2659 - val_accuracy: 0.9067\n",
            "Epoch 112/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2500 - accuracy: 0.9057 - val_loss: 0.2709 - val_accuracy: 0.9037\n",
            "Epoch 113/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2507 - accuracy: 0.9033 - val_loss: 0.2628 - val_accuracy: 0.9060\n",
            "Epoch 114/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2499 - accuracy: 0.9038 - val_loss: 0.2636 - val_accuracy: 0.9045\n",
            "Epoch 115/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2512 - accuracy: 0.9025 - val_loss: 0.2623 - val_accuracy: 0.9038\n",
            "Epoch 116/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2493 - accuracy: 0.9051 - val_loss: 0.2625 - val_accuracy: 0.9057\n",
            "Epoch 117/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2476 - accuracy: 0.9051 - val_loss: 0.2666 - val_accuracy: 0.9025\n",
            "Epoch 118/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2474 - accuracy: 0.9053 - val_loss: 0.2660 - val_accuracy: 0.9043\n",
            "Epoch 119/400\n",
            "422/422 [==============================] - 13s 30ms/step - loss: 0.2478 - accuracy: 0.9049 - val_loss: 0.2632 - val_accuracy: 0.9047\n",
            "Epoch 120/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2469 - accuracy: 0.9057 - val_loss: 0.2661 - val_accuracy: 0.9035\n",
            "Epoch 121/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2454 - accuracy: 0.9059 - val_loss: 0.2649 - val_accuracy: 0.9042\n",
            "Epoch 122/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2452 - accuracy: 0.9065 - val_loss: 0.2630 - val_accuracy: 0.9065\n",
            "Epoch 123/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2491 - accuracy: 0.9045 - val_loss: 0.2713 - val_accuracy: 0.9043\n",
            "Epoch 124/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2468 - accuracy: 0.9059 - val_loss: 0.2695 - val_accuracy: 0.9047\n",
            "Epoch 125/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2496 - accuracy: 0.9053 - val_loss: 0.2671 - val_accuracy: 0.9050\n",
            "Epoch 126/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2465 - accuracy: 0.9052 - val_loss: 0.2600 - val_accuracy: 0.9032\n",
            "Epoch 127/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2429 - accuracy: 0.9075 - val_loss: 0.2700 - val_accuracy: 0.9048\n",
            "Epoch 128/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2436 - accuracy: 0.9062 - val_loss: 0.2656 - val_accuracy: 0.9068\n",
            "Epoch 129/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2458 - accuracy: 0.9064 - val_loss: 0.2663 - val_accuracy: 0.9043\n",
            "Epoch 130/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2402 - accuracy: 0.9076 - val_loss: 0.2623 - val_accuracy: 0.9065\n",
            "Epoch 131/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2448 - accuracy: 0.9064 - val_loss: 0.2661 - val_accuracy: 0.9045\n",
            "Epoch 132/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2415 - accuracy: 0.9077 - val_loss: 0.2625 - val_accuracy: 0.9057\n",
            "Epoch 133/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2403 - accuracy: 0.9079 - val_loss: 0.2610 - val_accuracy: 0.9063\n",
            "Epoch 134/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2417 - accuracy: 0.9071 - val_loss: 0.2610 - val_accuracy: 0.9062\n",
            "Epoch 135/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2413 - accuracy: 0.9074 - val_loss: 0.2673 - val_accuracy: 0.9055\n",
            "Epoch 136/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2435 - accuracy: 0.9072 - val_loss: 0.2641 - val_accuracy: 0.9062\n",
            "Epoch 137/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2404 - accuracy: 0.9084 - val_loss: 0.2687 - val_accuracy: 0.9028\n",
            "Epoch 138/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2409 - accuracy: 0.9074 - val_loss: 0.2647 - val_accuracy: 0.9052\n",
            "Epoch 139/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2436 - accuracy: 0.9076 - val_loss: 0.2613 - val_accuracy: 0.9065\n",
            "Epoch 140/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2412 - accuracy: 0.9076 - val_loss: 0.2617 - val_accuracy: 0.9060\n",
            "Epoch 141/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2419 - accuracy: 0.9080 - val_loss: 0.2643 - val_accuracy: 0.9058\n",
            "Epoch 142/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2412 - accuracy: 0.9069 - val_loss: 0.2624 - val_accuracy: 0.9078\n",
            "Epoch 143/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2391 - accuracy: 0.9087 - val_loss: 0.2656 - val_accuracy: 0.9065\n",
            "Epoch 144/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2402 - accuracy: 0.9079 - val_loss: 0.2669 - val_accuracy: 0.9063\n",
            "Epoch 145/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.2413 - accuracy: 0.9082 - val_loss: 0.2674 - val_accuracy: 0.9053\n",
            "Epoch 146/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2399 - accuracy: 0.9092 - val_loss: 0.2623 - val_accuracy: 0.9070\n",
            "Epoch 147/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2411 - accuracy: 0.9079 - val_loss: 0.2644 - val_accuracy: 0.9062\n",
            "Epoch 148/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2335 - accuracy: 0.9114 - val_loss: 0.2605 - val_accuracy: 0.9067\n",
            "Epoch 149/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2397 - accuracy: 0.9088 - val_loss: 0.2593 - val_accuracy: 0.9065\n",
            "Epoch 150/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2363 - accuracy: 0.9097 - val_loss: 0.2633 - val_accuracy: 0.9050\n",
            "Epoch 151/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2376 - accuracy: 0.9095 - val_loss: 0.2649 - val_accuracy: 0.9050\n",
            "Epoch 152/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2368 - accuracy: 0.9094 - val_loss: 0.2633 - val_accuracy: 0.9057\n",
            "Epoch 153/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2381 - accuracy: 0.9092 - val_loss: 0.2639 - val_accuracy: 0.9052\n",
            "Epoch 154/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2371 - accuracy: 0.9095 - val_loss: 0.2642 - val_accuracy: 0.9070\n",
            "Epoch 155/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2351 - accuracy: 0.9097 - val_loss: 0.2661 - val_accuracy: 0.9060\n",
            "Epoch 156/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2368 - accuracy: 0.9097 - val_loss: 0.2645 - val_accuracy: 0.9053\n",
            "Epoch 157/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2372 - accuracy: 0.9100 - val_loss: 0.2650 - val_accuracy: 0.9057\n",
            "Epoch 158/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2365 - accuracy: 0.9094 - val_loss: 0.2614 - val_accuracy: 0.9070\n",
            "Epoch 159/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2377 - accuracy: 0.9086 - val_loss: 0.2604 - val_accuracy: 0.9050\n",
            "Epoch 160/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2369 - accuracy: 0.9086 - val_loss: 0.2608 - val_accuracy: 0.9075\n",
            "Epoch 161/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2333 - accuracy: 0.9099 - val_loss: 0.2577 - val_accuracy: 0.9092\n",
            "Epoch 162/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2334 - accuracy: 0.9102 - val_loss: 0.2649 - val_accuracy: 0.9055\n",
            "Epoch 163/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2367 - accuracy: 0.9097 - val_loss: 0.2651 - val_accuracy: 0.9062\n",
            "Epoch 164/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2333 - accuracy: 0.9093 - val_loss: 0.2657 - val_accuracy: 0.9065\n",
            "Epoch 165/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2341 - accuracy: 0.9094 - val_loss: 0.2629 - val_accuracy: 0.9068\n",
            "Epoch 166/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2341 - accuracy: 0.9087 - val_loss: 0.2645 - val_accuracy: 0.9043\n",
            "Epoch 167/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2294 - accuracy: 0.9119 - val_loss: 0.2631 - val_accuracy: 0.9065\n",
            "Epoch 168/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2315 - accuracy: 0.9115 - val_loss: 0.2619 - val_accuracy: 0.9045\n",
            "Epoch 169/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2316 - accuracy: 0.9109 - val_loss: 0.2667 - val_accuracy: 0.9057\n",
            "Epoch 170/400\n",
            "422/422 [==============================] - 13s 30ms/step - loss: 0.2332 - accuracy: 0.9116 - val_loss: 0.2627 - val_accuracy: 0.9073\n",
            "Epoch 171/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2342 - accuracy: 0.9119 - val_loss: 0.2604 - val_accuracy: 0.9075\n",
            "Epoch 172/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2327 - accuracy: 0.9099 - val_loss: 0.2622 - val_accuracy: 0.9037\n",
            "Epoch 173/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2335 - accuracy: 0.9104 - val_loss: 0.2629 - val_accuracy: 0.9078\n",
            "Epoch 174/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2315 - accuracy: 0.9112 - val_loss: 0.2615 - val_accuracy: 0.9043\n",
            "Epoch 175/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2299 - accuracy: 0.9125 - val_loss: 0.2640 - val_accuracy: 0.9058\n",
            "Epoch 176/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2312 - accuracy: 0.9118 - val_loss: 0.2611 - val_accuracy: 0.9060\n",
            "Epoch 177/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2267 - accuracy: 0.9135 - val_loss: 0.2608 - val_accuracy: 0.9050\n",
            "Epoch 178/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2318 - accuracy: 0.9115 - val_loss: 0.2629 - val_accuracy: 0.9058\n",
            "Epoch 179/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2297 - accuracy: 0.9125 - val_loss: 0.2587 - val_accuracy: 0.9070\n",
            "Epoch 180/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2314 - accuracy: 0.9112 - val_loss: 0.2605 - val_accuracy: 0.9085\n",
            "Epoch 181/400\n",
            "422/422 [==============================] - 11s 26ms/step - loss: 0.2301 - accuracy: 0.9123 - val_loss: 0.2603 - val_accuracy: 0.9080\n",
            "Epoch 182/400\n",
            "422/422 [==============================] - 11s 25ms/step - loss: 0.2316 - accuracy: 0.9107 - val_loss: 0.2608 - val_accuracy: 0.9077\n",
            "Epoch 183/400\n",
            "422/422 [==============================] - 12s 30ms/step - loss: 0.2323 - accuracy: 0.9111 - val_loss: 0.2673 - val_accuracy: 0.9053\n",
            "Epoch 184/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2287 - accuracy: 0.9128 - val_loss: 0.2654 - val_accuracy: 0.9070\n",
            "Epoch 185/400\n",
            "422/422 [==============================] - 11s 27ms/step - loss: 0.2301 - accuracy: 0.9125 - val_loss: 0.2589 - val_accuracy: 0.9073\n",
            "Epoch 186/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2274 - accuracy: 0.9130 - val_loss: 0.2604 - val_accuracy: 0.9082\n",
            "Epoch 187/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2286 - accuracy: 0.9127 - val_loss: 0.2627 - val_accuracy: 0.9057\n",
            "Epoch 188/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2312 - accuracy: 0.9101 - val_loss: 0.2599 - val_accuracy: 0.9082\n",
            "Epoch 189/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2288 - accuracy: 0.9122 - val_loss: 0.2639 - val_accuracy: 0.9082\n",
            "Epoch 190/400\n",
            "422/422 [==============================] - 12s 29ms/step - loss: 0.2317 - accuracy: 0.9112 - val_loss: 0.2591 - val_accuracy: 0.9082\n",
            "Epoch 191/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2267 - accuracy: 0.9130 - val_loss: 0.2645 - val_accuracy: 0.9060\n",
            "Epoch 192/400\n",
            "422/422 [==============================] - 12s 28ms/step - loss: 0.2288 - accuracy: 0.9113 - val_loss: 0.2644 - val_accuracy: 0.9050\n",
            "Epoch 193/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2300 - accuracy: 0.9128 - val_loss: 0.2577 - val_accuracy: 0.9048\n",
            "Epoch 194/400\n",
            "422/422 [==============================] - 12s 27ms/step - loss: 0.2266 - accuracy: 0.9129 - val_loss: 0.2596 - val_accuracy: 0.9063\n",
            "Epoch 195/400\n",
            "144/422 [=========>....................] - ETA: 8s - loss: 0.2250 - accuracy: 0.9111"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-08c403e7f9cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;31m# callbacks=[callback],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1811\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;34m\"\"\"Updates the progbar.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# One-indexed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks.py\u001b[0m in \u001b[0;36m_maybe_init_progbar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;31m# `self.stateful_metrics`. Remove \"cast to set\" when TF1 support is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# dropped.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nhistory dict:', list(history.history.keys()))"
      ],
      "metadata": {
        "id": "CxRE-B_Q6Sii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.evaluate(x_test, y_test)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "YXpwRRTvTBNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = load_model('best_model.h5')\n",
        "result = saved_model.evaluate(x_test, y_test)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "dpaZAW0vp1AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.models import load_model\n",
        "# saved_model = load_model('best_model.h5')\n",
        "# # _, train_acc = saved_model.evaluate(x_train, y_train, verbose=0)\n",
        "# _, test_acc = saved_model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('Test: %.3f' % (test_acc))"
      ],
      "metadata": {
        "id": "DR_MS9DABo_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(history_dict['loss']) + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n",
        "plt.ylim (0.2, 0.5)\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "1\n",
        "plt.clf()\n",
        "plt.figure(figsize=(10, 5))\n",
        "val_acc_values = history_dict['accuracy']\n",
        "plt.plot(epochs, history_dict['accuracy'], 'bo', label='Training acc')\n",
        "plt.plot(epochs, history_dict['val_accuracy'], 'r', label='Validation acc')\n",
        "plt.ylim (0.88, 0.92)\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "2Ll_5sGlXKMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probability_model = tf.keras.Sequential([model,\n",
        "                                         tf.keras.layers.Softmax()])"
      ],
      "metadata": {
        "id": "VzOdKAT_uV-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = probability_model.predict(x_test)"
      ],
      "metadata": {
        "id": "f4is3NFruYom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0]"
      ],
      "metadata": {
        "id": "82cigvlZudz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "6RsQPHyYuNGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "hnyetjn hrte()"
      ],
      "metadata": {
        "id": "S4hcxZttmp-0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}